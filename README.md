# Minimizing the cross entropy : a nice trip from Maximum likelihood to Kullback–Leibler divergence

The learning algorithm is often stated as a minimization problem where the objective function represent the goal to be reached. The objective function is also called loss function or cost function in machine learning. 

### We show that minimizing the cross entropy is equivalent to maximizing the likelihood in case of discriminative models.
### We show that minimizing the cross entropy is equivalent to minimizing the Kullback–Leibler divergence

### We show that the cross entropy loss makes the logistic regression problem convex
### We present an application to cats and dogs image classification

![Images](http://romain.raveaux.free.fr/document/catsanddgos.PNG)

HTML version of the notebook can be found here. HTML Notebook
http://romain.raveaux.free.fr/document/CrossEntropy.html


